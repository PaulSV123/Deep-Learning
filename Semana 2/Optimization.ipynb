{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "emAw6hhB8K1y",
        "outputId": "4c995cfb-f268-41eb-d2c7-d07e8bab67bd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 0: Loss = 55.6846\n",
            "Iter 100: Loss = 11.1339\n",
            "Iter 200: Loss = 9.8456\n",
            "Iter 300: Loss = 9.5654\n",
            "Iter 400: Loss = 8.7830\n",
            "Iter 500: Loss = 8.2365\n",
            "Iter 600: Loss = 7.5592\n",
            "Iter 700: Loss = 7.7278\n",
            "Iter 800: Loss = 7.5289\n",
            "Iter 900: Loss = 7.7323\n",
            "Iter 999: Loss = 7.1332\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import keras\n",
        "\n",
        "def svm_loss_and_grad(W, X, y, delta=1.0):\n",
        "    \"\"\"\n",
        "    W: pesos (C x D)\n",
        "    X: datos (D x N)\n",
        "    y: etiquetas (N,)\n",
        "    \"\"\"\n",
        "    num_classes = W.shape[0]\n",
        "    num_train = X.shape[1]\n",
        "\n",
        "    scores = W.dot(X)  # C x N\n",
        "    correct_scores = scores[y, np.arange(num_train)]  # (N,)\n",
        "    margins = np.maximum(0, scores - correct_scores + delta)\n",
        "    margins[y, np.arange(num_train)] = 0\n",
        "    loss = np.sum(margins) / num_train\n",
        "\n",
        "    # Gradiente\n",
        "    binary = margins > 0\n",
        "    binary = binary.astype(float)\n",
        "    row_sum = np.sum(binary, axis=0)\n",
        "    binary[y, np.arange(num_train)] = -row_sum\n",
        "    dW = binary.dot(X.T) / num_train  # (C x D)\n",
        "\n",
        "    return loss, dW\n",
        "\n",
        "# ------------------------------\n",
        "# Cargar y preparar datos\n",
        "# ------------------------------\n",
        "(Xtr, Ytr), (_, _) = keras.datasets.cifar10.load_data()\n",
        "Ytr = Ytr.flatten()\n",
        "\n",
        "Xtr_flat = Xtr.reshape(Xtr.shape[0], -1)\n",
        "Xtr_bias = np.hstack([Xtr_flat, np.ones((Xtr_flat.shape[0], 1))])  # Bias trick\n",
        "Xtr_T = Xtr_bias.T  # D x N\n",
        "Ytr = np.array(Ytr)\n",
        "\n",
        "# ------------------------------\n",
        "# Gradient Descent con Mini-batch\n",
        "# ------------------------------\n",
        "# Inicializar pesos\n",
        "np.random.seed(0)\n",
        "C, D = 10, Xtr_T.shape[0]\n",
        "W = 0.001 * np.random.randn(C, D)\n",
        "\n",
        "# Hiperparámetros\n",
        "num_iters = 1000\n",
        "batch_size = 512\n",
        "learning_rate = 1e-7\n",
        "\n",
        "loss_history = []\n",
        "\n",
        "for it in range(num_iters):\n",
        "    batch_indices = np.random.choice(Xtr_T.shape[1], batch_size, replace=False)\n",
        "    X_batch = Xtr_T[:, batch_indices]  # D x batch_size\n",
        "    y_batch = Ytr[batch_indices]       # batch_size\n",
        "\n",
        "    # Calcular pérdida y gradiente\n",
        "    loss, grad = svm_loss_and_grad(W, X_batch, y_batch)\n",
        "    W -= learning_rate * grad  # Actualización de pesos\n",
        "\n",
        "    loss_history.append(loss)\n",
        "    if it % 100 == 0 or it == num_iters - 1:\n",
        "        print(f\"Iter {it}: Loss = {loss:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(_, _), (Xte, Yte) = keras.datasets.cifar10.load_data()\n",
        "Yte = Yte.flatten()\n",
        "\n",
        "Xte_flat = Xte.reshape(Xte.shape[0], -1)\n",
        "Xte_bias = np.hstack([Xte_flat, np.ones((Xte_flat.shape[0], 1))])  # Bias trick\n",
        "Xte_T = Xte_bias.T  # D x N_test (3073 x 10000)\n",
        "\n",
        "# Predecir usando los pesos entrenados\n",
        "scores_test = W.dot(Xte_T)  # shape: (10, 10000)\n",
        "Yte_pred = np.argmax(scores_test, axis=0)  # predicciones: (10000,)\n",
        "\n",
        "# Calcular accuracy\n",
        "test_accuracy = np.mean(Yte_pred == Yte)\n",
        "print(f\"\\nTest accuracy: {test_accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7WnkzOx48pz3",
        "outputId": "60a1c2c7-7c8a-475d-b56e-a28f0ab17aeb"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test accuracy: 0.2739\n"
          ]
        }
      ]
    }
  ]
}